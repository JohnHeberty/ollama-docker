# Ollama + FastAPI - Sistema Simplificado

Sistema completo para rodar modelos LLM localmente com Ollama e FastAPI como proxy transparente.

## ğŸ¯ Funcionalidades

- **Ollama**: Servidor LLM rodando em container
- **FastAPI Proxy**: API que atua como proxy transparente para o Ollama
- **Prompts Externos**: Controle total dos prompts fora da aplicaÃ§Ã£o
- **Conversa Direta**: ComunicaÃ§Ã£o direta com os modelos via API

## ğŸš€ InÃ­cio RÃ¡pido

### PrÃ©-requisitos
- Docker e Docker Compose
- Python 3.8+ (para o cliente)

### Uma Ãºnica linha para subir tudo:
```bash
# Do diretÃ³rio raiz
cd src && docker compose up -d

# Ou diretamente no diretÃ³rio src
docker compose up -d
```

### Com script automÃ¡tico:
```powershell
# Windows PowerShell - Script bÃ¡sico
.\src\start.ps1

# Windows PowerShell - Script otimizado (recomendado)
.\src\start_V2.ps1
```

### Alternativa via diretÃ³rio src:
```bash
cd src
docker compose up -d
```

## ğŸ“ Estrutura do Projeto

```
ollama-docker/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ Dockerfile              # Container da API FastAPI
â”‚   â”œâ”€â”€ compose.yml             # OrquestraÃ§Ã£o dos serviÃ§os Docker
â”‚   â”œâ”€â”€ app.py                  # FastAPI Proxy (transparente)
â”‚   â”œâ”€â”€ requirements.txt        # DependÃªncias Python
â”‚   â”œâ”€â”€ start.ps1               # Script bÃ¡sico de inicializaÃ§Ã£o
â”‚   â”œâ”€â”€ start_V2.ps1           # Script otimizado com recursos
â”‚   â””â”€â”€ test.json              # Arquivo para testes
â”œâ”€â”€ chat_client.py              # Cliente Python para teste
â”œâ”€â”€ example.ipynb              # Notebook Jupyter com exemplos
â”œâ”€â”€ LICENSE                    # LicenÃ§a MIT
â”œâ”€â”€ .gitignore                 # Arquivos ignorados pelo Git
â””â”€â”€ README.md                  # Este arquivo
```

## ğŸŒ Endpoints

### FastAPI Proxy (Porta 8000)
- `GET /` - Status geral da API
- `GET /health` - Status detalhado do sistema
- `GET /models` - Listar modelos disponÃ­veis (formato simplificado)
- `GET /modelos` - Listar modelos disponÃ­veis (formato completo)  
- `POST /chat` - Conversa com contexto e timeout configurÃ¡vel
- `POST /modelo/baixar` - Baixar novos modelos
- `GET /docs` - DocumentaÃ§Ã£o automÃ¡tica FastAPI
- `API /ollama/{path}` - Proxy genÃ©rico para qualquer endpoint Ollama

### Ollama Direto (Porta 11434)
- Acesso direto ao Ollama (opcional)

## ğŸ’» Uso via Cliente Python

```python
from chat_client import ChatClient

# Inicializar cliente
client = ChatClient()

# Verificar status
status = client.health_check()
print(status)

# Listar modelos
modelos = client.listar_modelos()
print(modelos)

# Conversar com timeout configurÃ¡vel
resposta = client.chat(
    "Explique o que Ã© transporte ferroviÃ¡rio",
    modelo="tinyllama:latest",
    timeout=300  # 5 minutos
)
print(resposta)

# Baixar um modelo
resultado = client.baixar_modelo("qwen2:1.5b", timeout=1800)
print(resultado)
```

## ğŸ¨ Prompts Externos - Filosofia

O sistema foi projetado para **nÃ£o ter lÃ³gica de prompts internos**. Toda a inteligÃªncia de prompt fica **fora** da aplicaÃ§Ã£o:

```python
# Seus prompts personalizados
prompt_extracao = f"""
VocÃª Ã© um especialista em anÃ¡lise de dados de transporte.
Extraia TODOS os dados do texto: {texto}
Retorne em formato JSON estruturado.
"""

# Envio via proxy (sem modificaÃ§Ã£o) com timeout
resultado = client.chat(
    mensagem=prompt_extracao,
    modelo="tinyllama:latest",
    temperature=0.1,
    timeout=600  # 10 minutos
)
```

### Vantagens:
- âœ… **Controle total** sobre prompts
- âœ… **Flexibilidade mÃ¡xima** para diferentes casos de uso
- âœ… **FastAPI sÃ³ transita** dados sem modificar
- âœ… **Facilidade de manutenÃ§Ã£o** e teste
- âœ… **ReutilizaÃ§Ã£o** de prompts em diferentes contextos
- âœ… **Timeout configurÃ¡vel** por requisiÃ§Ã£o

## ğŸ”§ InstalaÃ§Ã£o de Modelos

```bash
# Via container direto (recomendado)
docker exec ollama-server ollama pull tinyllama

# Via API FastAPI (programaticamente)
curl -X POST http://localhost:8000/modelo/baixar \
  -H "Content-Type: application/json" \
  -d '{"name": "tinyllama:latest"}'

# Verificar modelos instalados
curl http://localhost:8000/models

# Modelos recomendados para baixo consumo
docker exec ollama-server ollama pull tinyllama:latest
docker exec ollama-server ollama pull qwen2:1.5b
docker exec ollama-server ollama pull deepcoder:1.5b
docker exec ollama-server ollama pull stablelm2:1.6b
```

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Seu Cliente   â”‚â”€â”€â”€â–¶â”‚  FastAPI Proxy  â”‚â”€â”€â”€â–¶â”‚     Ollama      â”‚
â”‚  (Prompts)      â”‚    â”‚  (Transparente) â”‚    â”‚   (Modelos)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â–²                           â”‚                       â”‚
      â”‚                           â”‚                       â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    Resposta sem modificaÃ§Ã£o
```

### Fluxo de Dados:
1. **Cliente** envia prompt personalizado
2. **FastAPI Proxy** repassa exatamente como recebido
3. **Ollama** processa com o modelo escolhido
4. **Resposta** retorna sem modificaÃ§Ãµes
5. **Cliente** recebe resposta pura do modelo

## â“ ResoluÃ§Ã£o de Problemas

### Container nÃ£o inicia
```bash
# Verificar logs
cd src
docker compose logs

# Reiniciar sistema
docker compose down && docker compose up -d
```

### Modelo nÃ£o responde ou timeout
```bash
# Verificar se modelo existe
curl http://localhost:8000/models

# Baixar modelo se necessÃ¡rio
docker exec ollama-server ollama pull tinyllama:latest

# Verificar recursos do sistema (use script otimizado)
.\src\start_V2.ps1
```

### API nÃ£o responde
```bash
# Verificar se containers estÃ£o rodando
docker ps

# Verificar saÃºde da API
curl http://localhost:8000/health

# Verificar logs do FastAPI
docker logs fastapi-proxy
```

### Para parar tudo
```bash
cd src
docker compose down
```

### OtimizaÃ§Ã£o de Performance
```bash
# Use o script otimizado que aloca recursos adequadamente
.\src\start_V2.ps1

# Pre-carrega modelos em memÃ³ria para melhor performance
# Configure variÃ¡veis de ambiente no compose.yml para sua mÃ¡quina
```

## ğŸ¯ Casos de Uso

### 1. ExtraÃ§Ã£o de Dados com Timeout
```python
prompt = f"Extraia dados tÃ©cnicos do texto: {texto_ferroviario}"
dados = client.chat(
    mensagem=prompt, 
    modelo="tinyllama:latest", 
    temperature=0.1,
    timeout=300
)
```

### 2. AnÃ¡lise de Viabilidade
```python
prompt = f"Analise viabilidade financeira: {dados_projeto}"
analise = client.chat(
    mensagem=prompt, 
    modelo="qwen2:1.5b", 
    temperature=0.3,
    timeout=600
)
```

### 3. GeraÃ§Ã£o de RelatÃ³rios
```python
prompt = f"Gere relatÃ³rio executivo: {dados_completos}"
relatorio = client.chat(
    mensagem=prompt, 
    modelo="tinyllama:latest", 
    temperature=0.2,
    timeout=900
)
```

## ğŸ““ Exemplo Interativo com Jupyter

O projeto inclui um notebook Jupyter (`example.ipynb`) com exemplos prÃ¡ticos:

- âœ… ConfiguraÃ§Ã£o automÃ¡tica do cliente
- âœ… VerificaÃ§Ã£o de modelos disponÃ­veis  
- âœ… Download automÃ¡tico de modelos recomendados
- âœ… Exemplos de anÃ¡lise de transporte ferroviÃ¡rio
- âœ… ExtraÃ§Ã£o de dados estruturados para JSON
- âœ… Testes com diferentes modelos de IA

```bash
# Abrir o notebook
jupyter notebook example.ipynb
```

## âš™ï¸ ConfiguraÃ§Ãµes AvanÃ§adas

### OtimizaÃ§Ã£o de Recursos
O arquivo `compose.yml` inclui configuraÃ§Ãµes para otimizar uso de CPU e RAM:

```yaml
environment:
  - OLLAMA_NUM_PARALLEL=6          # 6 processos paralelos
  - OLLAMA_MAX_LOADED_MODELS=2     # 2 modelos em memÃ³ria
  - OLLAMA_FLASH_ATTENTION=1       # OtimizaÃ§Ã£o de atenÃ§Ã£o
  - OLLAMA_NUM_THREAD=6            # 6 threads
```

### Scripts de InicializaÃ§Ã£o
- **start.ps1**: Script bÃ¡sico para inicializaÃ§Ã£o
- **start_V2.ps1**: Script otimizado com verificaÃ§Ã£o de recursos e prÃ©-carregamento

### Cliente Python AvanÃ§ado
O `chat_client.py` oferece:
- âœ… Timeout configurÃ¡vel por requisiÃ§Ã£o
- âœ… Download automÃ¡tico de modelos
- âœ… Tratamento de erros robusto
- âœ… InformaÃ§Ãµes detalhadas de performance

## ğŸ¤– Modelos Recomendados

### Modelos de Baixo Consumo (< 3GB RAM)
```bash
# Ultra-leve e rÃ¡pido (1.1B parÃ¢metros)
docker exec ollama-server ollama pull tinyllama:latest

# Equilibrio entre velocidade e qualidade (1.5B parÃ¢metros)  
docker exec ollama-server ollama pull qwen2:1.5b
docker exec ollama-server ollama pull qwen3:1.7b

# Especializado em cÃ³digo
docker exec ollama-server ollama pull deepcoder:1.5b

# Modelo estÃ¡vel
docker exec ollama-server ollama pull stablelm2:1.6b

# Modelo conversacional
docker exec ollama-server ollama pull tinydolphin:latest
```

### Escolha por Caso de Uso:
- **Testes rÃ¡pidos**: `tinyllama:latest`
- **AnÃ¡lises precisas**: `qwen2:1.5b` ou `qwen3:1.7b` 
- **GeraÃ§Ã£o de cÃ³digo**: `deepcoder:1.5b`
- **ConversaÃ§Ã£o**: `tinydolphin:latest`

## ğŸ“Š Monitoramento

### Verificar Status do Sistema
```bash
# Status geral
curl http://localhost:8000/

# Status detalhado  
curl http://localhost:8000/health

# Modelos disponÃ­veis
curl http://localhost:8000/models
```

### Logs dos Containers
```bash
# Logs do Ollama
docker logs ollama-server

# Logs do FastAPI
docker logs fastapi-proxy

# Logs em tempo real
docker compose logs -f
```

## ğŸ“ LicenÃ§a

Este projeto estÃ¡ licenciado sob a **MIT License** - veja o arquivo [LICENSE](LICENSE) para detalhes.

## ğŸ¤ ContribuiÃ§Ã£o

ContribuiÃ§Ãµes sÃ£o bem-vindas! Por favor:

1. FaÃ§a um Fork do projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## ğŸ“ Suporte

- **Issues**: [GitHub Issues](https://github.com/JohnHeberty/ollama-docker/issues)
- **DocumentaÃ§Ã£o FastAPI**: http://localhost:8000/docs (quando rodando)
- **Ollama Official**: https://ollama.ai/

## ğŸ‰ Agradecimentos

- [Ollama](https://ollama.ai/) - Framework para LLMs locais
- [FastAPI](https://fastapi.tiangolo.com/) - Framework web moderno e rÃ¡pido
- [Docker](https://www.docker.com/) - Plataforma de containerizaÃ§Ã£o
