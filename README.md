# Ollama + FastAPI - Sistema LLM Local

Sistema para executar modelos de linguagem localmente usando Ollama em Docker com interface FastAPI, com
monitoramnto de desempenho e inicio automatico otimizado conforme hardware.

## Teste de Fogo

Os modelos recomendados neste `readme.md` foram testado em um notebook `tinkped` com `8Gb` de RAM sem placa de
video com suporte para `CUDA`, e seu o processador Ã© um `Ryzen 5 7535U`.

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Seu Cliente   â”‚â”€â”€â”€â–¶â”‚  FastAPI Proxy  â”‚â”€â”€â”€â–¶â”‚     Ollama      â”‚
â”‚  (Prompts)      â”‚    â”‚  (Transparente) â”‚    â”‚   (Modelos)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â–²                           â”‚                       â”‚
      â”‚                           â”‚                       â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    Resposta sem modificaÃ§Ã£o
```

### Fluxo de Dados:
1. **Cliente** envia prompt personalizado
2. **FastAPI Proxy** repassa exatamente como recebido
3. **Ollama** processa com o modelo escolhido
4. **Resposta** retorna sem modificaÃ§Ãµes
5. **Cliente** recebe resposta pura do modelo

## ğŸ“ Estrutura

```
ollama-docker/
â”œâ”€â”€ service/                    # ğŸ³ ServiÃ§o Docker
â”‚   â”œâ”€â”€ app.py                  # FastAPI proxy para Ollama
â”‚   â”œâ”€â”€ compose.yml             # OrquestraÃ§Ã£o Docker
â”‚   â”œâ”€â”€ Dockerfile              # Build da API
â”‚   â”œâ”€â”€ entrypoint.sh           # Script de inicializaÃ§Ã£o Ollama
â”‚   â”œâ”€â”€ requirements.txt        # DependÃªncias Python do serviÃ§o
â”‚   â”œâ”€â”€ start.ps1               # Script bÃ¡sico PowerShell
â”‚   â””â”€â”€ start_V2.ps1            # Script otimizado PowerShell
â”‚
â”œâ”€â”€ src/                        # ğŸ“Š Ferramentas Externas
â”‚   â”œâ”€â”€ chat_client.py          # Cliente Python para Jupyter
â”‚   â”œâ”€â”€ example.ipynb           # Notebook com exemplos
â”‚   â”œâ”€â”€ pdf_processor.py        # Processador de PDFs
â”‚   â””â”€â”€ data/                   # Dados do projeto
â”‚       â”œâ”€â”€ external/           # PDFs originais
â”‚       â”œâ”€â”€ interim/            # Dados processados
â”‚       â””â”€â”€ processed/          # Resultados finais
â”‚
â”œâ”€â”€ LICENSE                     # LicenÃ§a MIT
â””â”€â”€ README.md                   # Esta documentaÃ§Ã£o
```

## ğŸ¯ Funcionalidades

### ğŸ³ ServiÃ§o Docker (`/service/`)
- **Ollama Server**: Modelos LLM rodando em container isolado
- **FastAPI Proxy**: API RESTful para comunicaÃ§Ã£o externa
- **Auto-configuraÃ§Ã£o**: Scripts PowerShell para inicializaÃ§Ã£o automÃ¡tica
- **Recursos Otimizados**: ConfiguraÃ§Ã£o dinÃ¢mica baseada no hardware

### ğŸ“Š Ferramentas Externas (`/src/`)
- **Cliente Python**: Biblioteca para integraÃ§Ã£o com Jupyter/Python
- **Processador PDF**: ExtraÃ§Ã£o de dados de documentos ferroviÃ¡rios
- **Notebook Interativo**: Exemplos prÃ¡ticos de uso
- **Gerenciamento de Dados**: Estrutura organizada para projetos

## ğŸš€ Setup RÃ¡pido

### PrÃ©-requisitos
- Docker Desktop instalado e rodando
- Python 3.8+ (para usar ferramentas em `/src/`)
- PowerShell (para scripts automatizados)

#### âš™ï¸ ConfiguraÃ§Ã£o .env (OBRIGATÃ“RIA)

O arquivo `src/.env` Ã© **essencial** para o funcionamento correto:

```bash
# ConfiguraÃ§Ã£o padrÃ£o (src/.env)
FASTAPI_URL=http://localhost:8000
OLLAMA_URL=http://localhost:11434
DEFAULT_CHAT_TIMEOUT=300
DEFAULT_DOWNLOAD_TIMEOUT=1800
RECOMMENDED_MODELS=tinyllama:latest,qwen2:1.5b,phi:latest
PDF_MAX_PAGES=100
PDF_TIMEOUT=600
DEBUG=True
VERBOSE_LOGGING=True
```

### 1. Configurar ferramentas Python
```bash
cd src
pip install -r requirements.txt
copy .env.example .env
```

### 2. Iniciar serviÃ§os Docker
```powershell
.\service\start_V2.ps1
```

### 3. Testar
```bash
curl http://localhost:8000/health
```

## ğŸŒ Endpoints da API (Porta 8000)
### Configurado no chat_client.py

| MÃ©todo HTTP | Endpoint              | DescriÃ§Ã£o                                   |
|-------------|-----------------------|---------------------------------------------|
| `GET`       | `/`                   | Status geral da API                         |
| `GET`       | `/health`             | VerificaÃ§Ã£o detalhada do sistema            |
| `GET`       | `/models`             | Lista modelos disponÃ­veis (formato simplificado) |
| `GET`       | `/modelos`            | Lista modelos com detalhes completos        |
| `POST`      | `/chat`               | Conversa com contexto e timeout configurÃ¡vel |
| `POST`      | `/modelo/baixar`      | Download de novos modelos                   |
| `GET`       | `/docs`               | DocumentaÃ§Ã£o automÃ¡tica FastAPI             |
| `GET`       | `/ollama/{path}`      | Proxy genÃ©rico para qualquer endpoint Ollama |

### Modelos Recomendados (Baixo Consumo < 4GB RAM)
```bash
# Dentro do container Ollama
docker exec ollama-server ollama pull tinyllama:latest     # 637MB - Ultra rÃ¡pido
docker exec ollama-server ollama pull tinydolphin:latest   # 637MB - Ultra rÃ¡pido
docker exec ollama-server ollama pull qwen2:1.5b           # 934MB - Equilibrado  
docker exec ollama-server ollama pull deepcoder:1.5b       # 
docker exec ollama-server ollama pull qwen3:1.7b           # 1.6GB - Microsoft

# Via API (programaticamente)
client.baixar_modelo("tinyllama:latest")
client.baixar_modelo("tinydolphin:latest")
client.baixar_modelo("qwen2:1.5b")
client.baixar_modelo("deepcoder:1.5b")
client.baixar_modelo("qwen3:1.7b ")
```

### Escolha modelo por Caso de Uso:
- **Testes rÃ¡pidos**: `tinyllama:latest`
- **AnÃ¡lises precisas**: `qwen2:1.5b` ou `qwen3:1.7b` 
- **GeraÃ§Ã£o de cÃ³digo**: `deepcoder:1.5b`
- **ConversaÃ§Ã£o**: `tinydolphin:latest`

### MÃ©tricas de Performance
```python
# Tempo de resposta e tokens
resultado = client.chat("Teste", timeout=60)
print(f"Tempo: {resultado['tempo_resposta']}s")
print(f"Tokens gerados: {resultado['tokens_gerados']}")
print(f"Tokens do prompt: {resultado['tokens_prompt']}")
```

## ğŸ’» Como Usar

### No Jupyter Notebook
```python
import sys
sys.path.append('../src')
from chat_client import ChatClient

client = ChatClient()  # Usa configuraÃ§Ãµes do .env automaticamente

# Chat simples
resposta = client.chat("Explique IA em uma frase")
print(resposta['resposta'])

# Listar modelos
modelos = client.listar_modelos()
print(modelos)

# Baixar modelo
client.baixar_modelo("tinyllama:latest")
```

### Processamento de PDFs
```python
from pdf_processor import PDFReader

pdf = PDFReader("data/external/documento.pdf")
texto = pdf.extract_text_optimized()

# Analisar com LLM
resultado = client.chat(f"Resuma este texto: {texto[:2000]}")
```

## âš™ï¸ ConfiguraÃ§Ã£o (.env)

Arquivo `src/.env` com configuraÃ§Ãµes principais:
```bash
FASTAPI_URL=http://localhost:8000
DEFAULT_CHAT_TIMEOUT=300
DEFAULT_DOWNLOAD_TIMEOUT=1800
RECOMMENDED_MODELS=tinyllama:latest,qwen2:1.5b
```

## â“ ResoluÃ§Ã£o de Problemas

### Container nÃ£o inicia
```bash
# Verificar logs
docker-compose logs

# Recriar containers
docker-compose down --volumes
docker-compose up --build -d

# Verificar recursos
docker stats
```

### API nÃ£o responde
```bash
# Verificar portas
netstat -an | findstr :8000
netstat -an | findstr :11434

# Testar conectividade
curl -v http://localhost:8000/health
curl -v http://localhost:11434/api/tags
```

### Modelo lento ou timeout
```python
# Aumentar timeout
client.chat("pergunta", timeout=600)  # 10 minutos

# Usar modelo mais leve
client.chat("pergunta", modelo="tinyllama:latest")

# Verificar recursos do sistema
docker stats ollama-server
```

## ğŸš¦ Comandos Ãšteis

### Docker
```bash
# Parar tudo
docker-compose down

# Reiniciar serviÃ§os
docker-compose restart

# Ver recursos em tempo real
docker stats

# Limpar sistema
docker system prune -a
```

### Python/Jupyter
```python
# Recarregar mÃ³dulo modificado
import importlib
importlib.reload(chat_client)

# Verificar versÃ£o das dependÃªncias  
import requests
print(requests.__version__)
```

## ğŸ”§ InstalaÃ§Ã£o de Modelos

```bash
# Via container direto (recomendado)
docker exec ollama-server ollama pull tinyllama

# Via API FastAPI (programaticamente)
curl -X POST http://localhost:8000/modelo/baixar \
  -H "Content-Type: application/json" \
  -d '{"name": "tinyllama:latest"}'

# Verificar modelos instalados
curl http://localhost:8000/models

# Modelos recomendados para baixo consumo
docker exec ollama-server ollama pull tinyllama:latest
docker exec ollama-server ollama pull qwen2:1.5b
docker exec ollama-server ollama pull deepcoder:1.5b
docker exec ollama-server ollama pull qwen3:1.7b
```

## ğŸ“Š Monitoramento

### Verificar Status do Sistema
```bash
# Status geral
curl http://localhost:8000/

# Status detalhado  
curl http://localhost:8000/health

# Modelos disponÃ­veis
curl http://localhost:8000/models
```

### Logs dos Containers
```bash
# Logs do Ollama
docker logs ollama-server

# Logs do FastAPI
docker logs fastapi-proxy

# Logs em tempo real
docker compose logs -f
```

## ğŸ¤ ContribuiÃ§Ã£o

1. Fork o projeto
2. Crie uma branch (`git checkout -b feature/nova-funcionalidade`)
3. Commit suas mudanÃ§as (`git commit -am 'Adiciona nova funcionalidade'`)
4. Push para a branch (`git push origin feature/nova-funcionalidade`)
5. Abra um Pull Request

## ğŸ“ Suporte e Links

- **DocumentaÃ§Ã£o API**: http://localhost:8000/docs (quando rodando)
- **GitHub Issues**: Para reportar bugs e sugestÃµes
- **Ollama Official**: https://ollama.ai/
- **FastAPI Docs**: https://fastapi.tiangolo.com/

## ğŸ“ LicenÃ§a

Este projeto estÃ¡ licenciado sob a **MIT License** - veja o arquivo [LICENSE](LICENSE) para detalhes.
---

## ğŸ‰ Agradecimentos

- [Ollama](https://ollama.ai/) - Framework para LLMs locais
- [FastAPI](https://fastapi.tiangolo.com/) - Framework web moderno
- [Docker](https://www.docker.com/) - Plataforma de containerizaÃ§Ã£o
resultado = client.baixar_modelo("qwen2:1.5b", timeout=1800)
print(resultado)
